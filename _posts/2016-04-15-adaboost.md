---
layout: post
title: adaboost及提升树算法学习
date: 2016-4-15
categories: blog
tags: [机器学习]
description: 
---
### adaboost
采用启发式的方式，通过将多个弱分类器组合成强分类器，来达到训练高性能模型的目的。
其中的组合是通过改变训练数据的权值分布实现的。

1. 初始化训练数据权值分布wi = 1/N
2. 得到基本分类器Gm
3. 计算误差e = wi * I( Gi(xi)≠yi )
4. 计算Gm的系数 α = 1/2log(1-e/e)
5. 根据α调整wi，wi+1 = wi * exp(-α * yi * Gm(xi)) 
   ，即分类正确时是-α，错误时是α，提高分类错误的类别的权重，降低分类正确的类别的权重。
6. 重复上述过程，根据新的wi+1训练再训练分类器。
7. 最后的分类函数是f(x) = ∑α*Gm

### 提升树
提升树以决策树为基本函数，被认为是统计学习中性能最好的方法之一

先介绍**前向分步算法**：

假设加法模型f(x) = β1Z1(x，ρ1) + β2Z2(x，ρ2) + β3Z3(x，ρ3)

其中β是系数，ρ是函数参数。

前向分步算法是从前往后，每次只优化一组β和ρ，最终达到整个f(x)的优化。

1. f1(x) = β1Z1(x，ρ1)  ,优化β1和ρ1
2. f2(x) = f1(x) + β2Z2(x，ρ2)  ,优化β2和ρ2
3. f3(x) = f2(x) + β3Z3(x，ρ3)  ,优化β3和ρ3

提升树利用加法模型，每一步优化一个决策树模型

 - f0(x) = 0

  - fm(x) = fm-1(x) + T(x,θ)
  
  当采用均方误差时，计算如下：
  
  - L = [y-fm(x)]² = [y-fm-1(x) - T(x,θ)]² = [r-T(x,θ)]²
  
  其中r=y-fm-1(x)是残差。
  所以除了第一次是用原始数据，每一步都根据残差来训练新的决策树分类器即可。
  
  
  
  
  
  







