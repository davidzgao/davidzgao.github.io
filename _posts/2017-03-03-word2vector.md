---
layout: post
title: Word2Vector学习笔记
date: 2017-3-3
categories: blog
tags: [机器学习,NLP]
description: Word2Vector学习笔记 
---
谷歌提出的方法，属于神经网络在NLP领域的使用，也是深度学习在NLP方面的一个发展。

### 传统词向量 one-hot模型
n个词，用n维向量表示。举个例子：

- “话筒”表示为 [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 …]

- “麦克”表示为 [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 …]

存在的问题：

1, 词汇鸿沟。即无法反应词汇间的相似度。相似度都为0

2, 维数灾难。矩阵太过稀疏，无法计算。

### Distributed representation词向量表示

用低维实数向量表示

- [0.792, −0.177, −0.107, 0.109, −0.542, …]

解决了词汇鸿沟和维数灾难的问题。

### 词向量模型
    a)  LSA矩阵分解模型

　　采用线性代数中的奇异值分解方法，选取前几个比较大的奇异值所对应的特征向量将原矩阵映射到低维空间中，从而达到词矢量的目的。

　　b)  PLSA 潜在语义分析概率模型

　　从概率学的角度重新审视了矩阵分解模型，并得到一个从统计，概率角度上推导出来的和LSA相当的词矢量模型。

　　c)  LDA 文档生成模型

　　按照文档生成的过程，使用贝叶斯估计统计学方法，将文档用多个主题来表示。LDA不只解决了同义词的问题，还解决了一次多义的问题。目前训练LDA模型的方法有原始论文中的基于EM和 差分贝叶斯方法以及后来出现的Gibbs Samplings 采样算法。

　　d)  Word2Vector 模型

　　最近几年刚刚火起来的算法，通过神经网络机器学习算法来训练N-gram 语言模型，并在训练过程中求出word所对应的vector的方法。本文将详细阐述此方法的原理。

### word2vector思想


包含cbow和skip-gram两个模型。前者是通过关联的n-1个词预测1个词，后者是通过1个词预测周围的n-1个词。

![img1]({{ site.imgBaseUrl }}/word2vector1.png )

![img2]({{ site.imgBaseUrl }}/word2vector2.png )

假设总的不同词语个数为V

输入层： V个节点，随机初始化

隐藏层：假设个数为H，使用sigmoid函数或tanh函数即可

输出层：也是V

另外如果你认认真真的学过神经网络稀疏自编码的知识就会知道，如果将输入层和输出层之间再构建一层传递关系，更有利于提高该模型的准确度。


