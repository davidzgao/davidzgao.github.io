---
layout: post
title: svm算法学习
date: 2017-5-7
categories: blog
tags: [机器学习]
description: 
---

### 目标

找到具有最大间隔距离的分离超平面
最大间隔由支持向量确定，分为函数间隔和几何间隔（归一化后）

分隔超平面：**w**x + b
，点A到超平面距离  |(**w**A + b)|/|**w**|

目标是求最大化最小间隔：

argmax(w,b){min(n)( label*(**w**x+b)/|**w**|) }

判别函数：
对于点A，

f(A) = sign(**w**A+b)


### 求解

直接求解困难，所以采用拉格朗日乘子法，变换成对偶形式，这样也能自然引入核函数。

- 1/2min(α) ∑∑αi * αj * yi * yj（**xi * xj**）- ∑αi 

- ∑αi  * yi = 0

-  0 ≤ αi ≤ C  (C 又称惩罚系数，用于调节支持非线性情况)

为了高效求解，采用SMO序列最小最优化算法。即每次只随机更新两个α，增大一个减小一个。

SMO算法：

- 外层是迭代次数循环

- 每一次选择两个α优化，α的选择是启发式的，即选择0< α <C 的α，同时第二个α选择最大步长的α。不断迭代直到达标。

然后 w = ∑αi  * yi * xi    
    
   b = yj - ∑αi  * yi * （xi*xj）
   
故可知，上述表达式只依赖于 αi>0 的部分，因此 αi>0 的项对应的是支持向量。上式w解唯一，b解不唯一

### 引入核函数

背景：由于求解非线性问题时，难以直接找到分离超平面，所以先将特征空间映射到高维空间，再在高维空间求解线性问题，因此引入核函数做线性变换。

核函数  K(x,y) = F(x)·F(z)

在给定核函数的情况下，可以利用解线性分类的方法求解非线性问题，学习也是隐式地在高维空间进行。实际中往往根据经验直接选择核函数。

常用核函数：

高斯核： K(x,z) = exp(-|x-z|²/2ρ²)http://www.cnblogs.com/jerrylead/archive/2011/03/18/1988419.html
多项式核： K(x,z) = (x·z +1)^p

### 其他人总结
[引自](http://www.cnblogs.com/jerrylead/archive/2011/03/18/1988419.html)

起初让我最头疼的是拉格朗日对偶和SMO，后来逐渐明白拉格朗日对偶的重要作用是将w的计算提前并消除w，使得优化函数变为拉格朗日乘子的单一参数优化问题。而SMO里面迭代公式的推导也着实让我花费了不少时间。

对比这么复杂的推导过程，SVM的思想确实那么简单。它不再像logistic回归一样企图去拟合样本点（中间加了一层sigmoid函数变换），而是就在样本中去找分隔线，为了评判哪条分界线更好，引入了几何间隔最大化的目标。

之后所有的推导都是去解决目标函数的最优化上了。在解决最优化的过程中，发现了w可以由特征向量内积来表示，进而发现了核函数，仅需要调整核函数就可以将特征进行低维到高维的变换，在低维上进行计算，实质结果表现在高维上。由于并不是所有的样本都可分，为了保证SVM的通用性，进行了软间隔的处理，导致的结果就是将优化问题变得更加复杂，然而惊奇的是松弛变量没有出现在最后的目标函数中。最后的优化求解问题，也被拉格朗日对偶和SMO算法化解，使SVM趋向于完美。







