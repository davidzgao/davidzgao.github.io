---
layout: post
title: adaboost及提升树，GBDT总结
date: 2017-7-15
categories: blog
tags: [机器学习]
description: 
---
### adaboost
采用启发式的方式，通过将多个弱分类器组合成强分类器，来达到训练高性能模型的目的。
其中的组合是通过改变训练数据的权值分布实现的。

1. 初始化训练数据权值分布wi = 1/N
2. 得到基本分类器Gm
3. 计算误差e = wi * I( Gi(xi)≠yi )
4. 计算Gm的系数 α = 1/2log(1-e/e)
5. 根据α调整wi，wi+1 = wi * exp(-α * yi * Gm(xi)) 
   ，即分类正确时是-α，错误时是α，提高分类错误的类别的权重，降低分类正确的类别的权重。
6. 重复上述过程，根据新的wi+1训练再训练分类器。
7. 最后的分类函数是f(x) = ∑α*Gm

### 提升树
提升树以决策树为基本函数，被认为是统计学习中性能最好的方法之一

先介绍**前向分步算法**：

假设加法模型f(x) = β1Z1(x，ρ1) + β2Z2(x，ρ2) + β3Z3(x，ρ3)

其中β是系数，ρ是函数参数。

前向分步算法是从前往后，每次只优化一组β和ρ，最终达到整个f(x)的优化。

1. f1(x) = β1Z1(x，ρ1)  ,优化β1和ρ1
2. f2(x) = f1(x) + β2Z2(x，ρ2)  ,优化β2和ρ2
3. f3(x) = f2(x) + β3Z3(x，ρ3)  ,优化β3和ρ3

提升树利用加法模型，每一步优化一个决策树模型

 - f0(x) = 0

  - fm(x) = fm-1(x) + T(x,θ)
  
  当采用均方误差时，计算如下：
  
  - L = [y-fm(x)]² = [y-fm-1(x) - T(x,θ)]² = [r-T(x,θ)]²
  
  其中r=y-fm-1(x)是残差。
  所以除了第一次是用原始数据，每一步都根据残差来训练新的决策树分类器即可。
  
  算法如下：
  
  ![bdt_algrm]({{ site.imgBaseUrl }}/boost/2.png)
  
  
  **例**：训练一个提升树模型来预测年龄：训练集是4个人，A，B，C，D年龄分别是14，16，24，26。样本中有购物金额、上网时长、经常到百度知道提问等特征。提升树的过程如下：
  
  ![bdt]({{ site.imgBaseUrl }}/boost/1.jpg)
  
  
  
### GBDT

GBDT(Gradient Boosting Decision Tree) 顾名思义就是梯度提升树。 基函数是回归决策树。

与提升树的区别就是损失函数不是均方损失，此时利用损失函数梯度下降的值作为残差，来迭代训练新的决策树。

  
  
![gbdt]({{ site.imgBaseUrl }}/boost/3.png)
  
1. 初始化，估计使损失函数极小化的常数值，它是只有一个根节点的树，即f0是一个常数值，在提升树里值为0。
2. 计算损失函数的负梯度在当前模型的值，将它作为残差的估计

3. 找到合适的间隔区域，以拟合残差
4. 根据间隔区域，利用线性搜索得到新的决策树，当前使损失函数极小化，
5. 更新回归树，fm = fm-1 + 残差
6. 不断重复2-5，直到满足要求，输出最终模型 f(x)= fM


  
  







