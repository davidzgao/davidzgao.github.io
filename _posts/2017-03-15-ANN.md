---
layout: post
title: 人工神经网络学习
date: 2017-3-15
categories: blog
tags: [机器学习,人工神经网络]
description:

---
包含输入层，隐藏层，输出层

写出递推公式  Z(n+1) = f(Z(n)*Wn)

训练：根据训练集，设x是输入，t是目标输出，o是实际输出

风险函数是求最小均方误差，根据梯度下降法

可以推导出利用t，o和x来表示，并迭代求解

w = w + delta（w）

delta（wi）=l* sum[（ti-oi）xi]

其中l是步长

这样，每次更新都要求sum，所以为了提高速度，产生了随机梯度下降，降低计算量

对于多层前馈网络，利用反向传播算更新wi（初始时，wi是随机的一个小数），即先求出x输入对应的实际输出o，然后根据o和t的差，求出输出层的误差，然后一层层反向传播，不断反向求前一层的误差，根据误差推导出每一层新的wi

**输出层计算**

通过更新权重和反映网络预测误差的偏倚，向后传播误差。对于输出层单元j，误差Errj用下式计算：


![img1]({{ site.imgBaseUrl }}/ann/1.gif)

其中Oj是单元j的实际输出，而Tj是j基于给定训练元组的已知目标值。注意，Oj * (1-Oj)是Logistic函数的导数。

**隐藏层计算**

为计算隐藏层单元j的误差，考虑到下一层中链接到j的单元的误差加权和。隐藏层单元j的误差是：

![img2]({{ site.imgBaseUrl }}/ann/2.gif)


其中，Wjk是由下一较高层中单元k到单元j的连接权重，而Errk是单元k的误差。

更新权重和偏倚，以反映传播误差，权重由下式更新，其中，delta Wij是权重Wij的改变

![img3]({{ site.imgBaseUrl }}/ann/3.gif)

![img4]({{ site.imgBaseUrl }}/ann/4.gif)

上面公式中的变量l是学习率，通常取0.0~1.0之间的常数值。如果学习率太小，学习将进行的很慢，如果学习率太大，则可能出现在不适当的解之间摆动。一个经验的法则是将学习率设置为1/t，t是当前训练集迭代的次数。

偏倚由下式更新。

![img5]({{ site.imgBaseUrl }}/ann/5.gif)

![img6]({{ site.imgBaseUrl }}/ann/6.gif)

注意，这里我们每处理一个元组就更新权重和偏倚，这称作实例更新（Case Update），权重和偏倚的增量也可以累计到变量中，可以在处理完训练集中的所有元组之后再进行权重和偏倚的更新，这种策略称之为周期更新（Epoch Update），扫描训练集的一次迭代是一个周期。理论上，后向传播的数学推导使用周期更新，而实践中实例更新更为常见，因为它通常产生更准确的结果。




##### 存在的问题：
对于梯度下降，可能无法找到全局最优，只能达到局部最优

- 更新时加入一个冲量，是上一次迭代的结果：delta(Wn) = A + l*delta(Wn-1)

- 采用随机梯度下降，不断变化的维度上可能能避免某一个局部最优

- 采用多次随机初始值训练，如果训练的模型不同，最后可以取各模型的平均值

- 学习速率l的选择，不是统一的，可以动态，例如l(n)= l(1)/sqrt(n)


##### 停止条件

可以用迭代次数或者训练误差低于某个值作为停止条件，但是这些都不够准确。最好的是准备一个测试集，看在测试集上的效果达到最佳或趋于稳定时停止迭代。以避免过拟合。

当数据量较小时可以使用交叉验证，先通过交叉验证确定迭代次数i，再在整个训练集上重新训练一次

##### tips

- 选择多组初始值进行尝试

- 隐藏层的选择也没有绝对标准，对层数和每层的节点数进行多次尝试，以达到最优效果









