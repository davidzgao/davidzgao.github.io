---
layout: post
title: 人工神经网络学习
date: 2017-3-15
categories: blog
tags: [机器学习,人工神经网络]
description:

---
包含输入层，隐藏层，输出层

写出递推公式  Z(n+1) = f(Z(n)*Wn)

训练：根据训练集，设x是输入，t是目标输出，o是实际输出

风险函数是求最小均方误差，根据梯度下降法

可以推导出利用t，o和x来表示，并迭代求解

w = w + delta（w）

delta（wi）=l* sum[（ti-oi）xi]

其中l是步长

这样，每次更新都要求sum，所以为了提高速度，产生了随机梯度下降，降低计算量

对于多层前馈网络，利用反向传播算更新wi（初始时，wi是随机的一个小数），即先求出x输入对应的实际输出o，然后根据o和t的差，求出输出层的误差，然后一层层反向传播，不断反向求前一层的误差，根据误差推导出每一层新的wi

##### 存在的问题：
对于梯度下降，可能无法找到全局最优，只能达到局部最优

- 更新时加入一个冲量，是上一次迭代的结果：delta(Wn) = A + l*delta(Wn-1)

- 采用随机梯度下降，不断变化的维度上可能能避免某一个局部最优

- 采用多次随机初始值训练，如果训练的模型不同，最后可以取各模型的平均值

##### 停止条件

可以用迭代次数或者训练误差低于某个值作为停止条件，但是这些都不够准确。最好的是准备一个测试集，看在测试集上的效果达到最佳或趋于稳定时停止迭代。以避免过拟合。

当数据量较小时可以使用交叉验证，先通过交叉验证确定迭代次数i，再在整个训练集上重新训练一次





